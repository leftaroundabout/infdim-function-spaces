% easychair.tex,v 3.5 2017/03/15

\documentclass[a4paper]{easychair}
%\documentclass[EPiC]{easychair}
%\documentclass[EPiCempty]{easychair}
%\documentclass[debug]{easychair}
%\documentclass[verbose]{easychair}
%\documentclass[notimes]{easychair}
%\documentclass[withtimes]{easychair}
%\documentclass[a4paper]{easychair}
%\documentclass[letterpaper]{easychair}

\usepackage{doc}
\usepackage{xspace}

\usepackage{fontspec}

\usepackage{amsmath}
\usepackage{amsfonts}

%\makeindex

%% Front Matter
%%
% Regular title as in the article class.
%
\title{Towards better data structures for numerics such as optimal transport}

% Authors are joined by \and. Their affiliations are given by \inst, which indexes
% into the list defined using \institute
%
\author{
   Justus Sagemüller\inst{1}
\and
    Olivier Verdier\inst{1}
\and
    Volker Stolz\inst{1}
}

% Institutes for affiliations are also joined by \and,
\institute{
  Western Norway University of Applied Sciences, 
  Bergen, Norway\\
  \email{\{jsag,over,vsto\}@hvl.no}
 }

%  \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads. When processed by
% EasyChair, this command is mandatory: a document without \authorrunning
% will be rejected by EasyChair

\authorrunning{Sagemüller, Verdier and Stolz}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. When processed by
% EasyChair, this command is mandatory: a document without \titlerunning
% will be rejected by EasyChair
\titlerunning{Towards well-typed optimal transport of distributions}

\begin{document}

\maketitle

\begin{abstract}
  In machine learning, it is often necessary to compare data distributions. One way of doing that is \emph{Optimal transport}. OT is a useful tool for assessing the difference/divergence (\emph{Wasserstein} or \emph{Earth mover distance}) between probability distributions, histograms etc., or for interpolating between them, particularly in case of non-overlapping distributions.
  
  The \emph{Sinkhorn algorithm} is an efficient means of calculating OT for arbitrary metrics on the base space. However it is in practice carried out only on a \emph{discretised representation} of the distributions.
  
  We implement the Sinkhorn algorithm on a data structure which handles the infinite dimensionality of the continuous distribution space through lazy evaluation. Apart from safer, easier handling of what resolution is necessary, this includes the ability to express with \emph{types} the mathematical meaning of a function or distribution.
  
  Our Sinkhorn implementation does work, but floating-point instability is observed in certain cases. Interestingly, it depends on whether the distributions are considered as functions or dual-functions, a distinction that does not appear in the conventional a-priori finite-dimensional reduction. We discuss this and whether it is possible to use types to prevent such issues, or to warn about them.
\end{abstract}

\label{sec:introduction}
\noindent%
Probability distributions are are the foundation of statistics and its applications. In the discrete case, such distributions are readily represented by a concrete probability value for each possible event -- i.e. as a function from the set of events $X$ to probabilities $[0,1]$.
Many applications however require, at least conceptually, probability distribution on \emph{continuous} spaces $X$ (real intervals, Euclidean planes, manifolds etc.).
Standard procedure is to discretise such spaces to a finite-dimensional approximation and then carry out any computer algorithms on the resulting discrete space of distributions. 

The resulting view suggests that distributions can still be understood as functions on $X$, as \emph{probability density functions}. Indeed this view has some merit thanks to the Riesz-Fréchet representation theorem, but fundamentally, distributions are better understood as (normalised) function{al}s: as linear mappings from the space of functions $X\to\mathbb{R}$ to $\mathbb{R}$.
That includes in particular also discrete, point-distributions on the continuous space (the prototype being the \emph{Dirac distribution}, which expresses that all the events happen at 0 on the real line). Such distributions do not correspond to any function $X\to\mathbb{R}$, but they do correspond to functionals $(X\to\mathbb{R})\to\mathbb{R}$, namely pointwise evaluation.

\newcommand{\marg}[1]{\mathbb{P}_\mathrm{#1}}
A natural question to ask is, given two distributions $\marg r$ and $\marg g$, how similar or dissimilar they are. This is of immediate importance in machine learning. In the function-view of distributions, a naïve attempt would be to sum/integrate the point-wise difference between them ($\mathcal{L}^1$ difference); in practice the Kullback-Leibler\cite{Kullback_1951} divergence family is more common, including the Jensen-Shannon divergence which can be used as a proper \emph{metric} on the distribution space.
 All of those share the problem that they do not take the topology of the base space $X$ into account. In particular, point-distributions are almost always classified as infinitely far apart, even when the points lie arbitrarily close in $X$. This is for example a problem in generative adversarial networks, leading to mode collapse. Resolution-limit / smearing can avoid this, but at the obvious cost of loss of resolution and without addressing the underlying problem.
What can address it\cite{pmlr-v70-arjovsky17a} is switching to a metric that does consider the topology of $X$. The \emph{Wasserstein metric} or \emph{earth mover distance} measures how far the “mass” in the distribution $\marg r$ needs to be moved across $X$ in order to obtain $\marg g$. This movement process, with the minimum movement-distance, is called \emph{optimal transport}.


% The table of contents below is added for your convenience. Please do not use
% the table of contents if you are preparing your paper for publication in the
% EPiC Series or Kalpa Publications series

%\setcounter{tocdepth}{2}
%{\small
%\tableofcontents}

%\section{To mention}
%
%Processing in EasyChair - number of pages.
%
%Examples of how EasyChair processes papers. Caveats (replacement of EC
%class, errors).

%------------------------------------------------------------------------------
%\bibliographystyle{plain}
%\bibliographystyle{alpha}
%\bibliographystyle{unsrt}
\bibliographystyle{abbrv}

\bibliography{ref}


\end{document}

